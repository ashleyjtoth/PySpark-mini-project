{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46f4495a-0f0b-4341-865f-ae8113a098c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "VsNYwUleOhHC"
   },
   "source": [
    "# Install Spark\n",
    "\n",
    "The below comands will install spark locally in our colab instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46374f17-20fc-402d-b7b2-3d333a08e01c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ydPIDOS1OhHE",
    "outputId": "6cdfa5a7-cc18-43bd-99b0-656e54758ca4"
   },
   "outputs": [],
   "source": [
    "# Colab: Java + PySpark\n",
    "!apt-get update -y && apt-get install -y openjdk-17-jdk\n",
    "!pip install -q pyspark==3.5.4\n",
    "import os, subprocess\n",
    "os.environ[\"JAVA_HOME\"] = subprocess.check_output(\n",
    "    \"readlink -f /usr/bin/java | sed 's:/bin/java::'\", shell=True\n",
    ").decode().strip()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"adult-mllib\").getOrCreate()\n",
    "print(\"Spark:\", spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c97b0a2-00f9-46fa-8e3d-661a6a5ef59d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "7oFu5ZoQOhHE"
   },
   "source": [
    "## Exercise Overview\n",
    "In this exercise we will play with Spark [Datasets & Dataframes](https://spark.apache.org/docs/latest/sql-programming-guide.html#datasets-and-dataframes), some [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html#sql), and build a couple of binary classifiaction models using [Spark ML](https://spark.apache.org/docs/latest/ml-guide.html) (with some [MLlib](https://spark.apache.org/mllib/) too).\n",
    "<br><br>\n",
    "The set up and approach will not be too dissimilar to the standard type of approach you might do in [Sklearn](http://scikit-learn.org/stable/index.html). Spark has matured to the stage now where for 90% of what you need to do (when analysisng tabular data) should be possible with Spark dataframes, SQL, and ML libraries. This is where this exerise is mainly trying to focus.  \n",
    "<br>\n",
    "\n",
    "#### Links & References\n",
    "\n",
    "Some useful links and references of sources used in creating this exercise:\n",
    "\n",
    "**Note**: Right click and open as new tab!\n",
    "<br>\n",
    "1. [Latest Spark Docs](https://spark.apache.org/docs/latest/index.html)\n",
    "1. [Databricks Homepage](https://databricks.com/)\n",
    "1. [Databricks Community Edition FAQ](https://databricks.com/product/faq/community-edition)\n",
    "1. [Databricks Self Paced Training](https://databricks.com/training-overview/training-self-paced)\n",
    "1. [Databricks Notebook Guide](https://docs.databricks.com/user-guide/notebooks/index.html)\n",
    "1. [Databricks Binary Classification Tutorial](https://docs.databricks.com/spark/latest/mllib/binary-classification-mllib-pipelines.html#binary-classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "695a1fed-7ad1-4a91-bd70-46e0275a793e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "fNxZiHwwOhHF"
   },
   "source": [
    "#### Get Data\n",
    "\n",
    "Here we will pull in some sample data from the UCI data repository.\n",
    "\n",
    "Feel free to adapt this notebook later to play around with a different dataset if you like (all available are listed in a cell below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e77b537c-c527-44fe-82e9-f6f7bdd54ea6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V0BslTH7OhHF",
    "outputId": "635287c8-273b-4a57-8947-3e00f84e2977"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .master(\"local[*]\")\n",
    "        .appName(\"adult-colab\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "import pandas as pd\n",
    "cols = [\n",
    "    \"age\",\"workclass\",\"fnlwgt\",\"education\",\"education_num\",\"marital_status\",\n",
    "    \"occupation\",\"relationship\",\"race\",\"sex\",\"capital_gain\",\"capital_loss\",\n",
    "    \"hours_per_week\",\"native_country\",\"income\"\n",
    "]\n",
    "pdf = pd.read_csv(\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\",\n",
    "    header=None,\n",
    "    names=cols,\n",
    "    skipinitialspace=True,\n",
    "    na_values=[\"?\", \" ?\"]\n",
    ")\n",
    "for c in pdf.select_dtypes(include=\"object\").columns:\n",
    "    pdf[c] = pdf[c].str.strip()\n",
    "pdf = pdf.dropna().reset_index(drop=True)\n",
    "sdf = spark.createDataFrame(pdf)\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS main\")\n",
    "spark.sql(\"USE main\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS adult\")\n",
    "sdf.createOrReplaceTempView(\"adult_src\")\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE adult\n",
    "USING PARQUET\n",
    "AS SELECT * FROM adult_src\n",
    "\"\"\")\n",
    "spark.sql(\"CREATE OR REPLACE TEMP VIEW adult AS SELECT * FROM adult\")\n",
    "spark.sql(\"SELECT COUNT(*) AS n_rows FROM adult\").show()\n",
    "spark.sql(\"SELECT * FROM adult LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84d3180c-0f4b-4375-8f97-7975ca99ef77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "qS9JZhdtOhHF"
   },
   "source": [
    "#### Spark SQL\n",
    "Below we will use Spark SQL to load in the data and then register it as a Dataframe aswell. So the end result will be a Spark SQL table called 'adult' and a Spark Dataframe called 'df_adult'.\n",
    "<br><br>\n",
    "This is an example of the flexibility in Spark in that you could do lots of you ETL and data wrangling using either Spark SQL or Dataframes and pyspark. Most of the time it's a case of using whatever you are most comfortable with.\n",
    "<br><br>\n",
    "When you get more advanced then you might consider learning about the pro's and con's of each and in what circumstances you might favour one over the other (or operating direclty on RDD's), [here](https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html) is a good article on the issues. For now, no need to overthink it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b73f000c-7068-4d9c-a559-18dc7bda95f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hDbZJSDDOhHF",
    "outputId": "96c0b6d6-e343-4ca6-8f95-882e2197ed70"
   },
   "outputs": [],
   "source": [
    "# look at the data\n",
    "spark.sql(\"SELECT * FROM adult LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b41c5e9-3101-41ba-a7e9-9e84399f3150",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "OE51stVZOhHG"
   },
   "source": [
    "If you are more comfortable with SQL then as you can see below, its very easy to just get going with writing standard SQL type code to analyse your data, do data wrangling and create new dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1219e3d2-a3ff-4d81-b688-c6751f950822",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f5quCZziOhHG",
    "outputId": "f7f95195-d8f0-4e3a-a7c9-41522e50ad3f"
   },
   "outputs": [],
   "source": [
    "# Lets get some summary marital status rates by occupation\n",
    "result = spark.sql(\n",
    "  \"\"\"\n",
    "  SELECT\n",
    "    occupation,\n",
    "    SUM(1) as n,\n",
    "    ROUND(AVG(if(LTRIM(marital_status) LIKE 'Married-%',1,0)),2) as married_rate,\n",
    "    ROUND(AVG(if(lower(marital_status) LIKE '%widow%',1,0)),2) as widow_rate,\n",
    "    ROUND(AVG(if(LTRIM(marital_status) = 'Divorced',1,0)),2) as divorce_rate,\n",
    "    ROUND(AVG(if(LTRIM(marital_status) = 'Separated',1,0)),2) as separated_rate,\n",
    "    ROUND(AVG(if(LTRIM(marital_status) = 'Never-married',1,0)),2) as bachelor_rate\n",
    "  FROM\n",
    "    adult\n",
    "  GROUP BY 1\n",
    "  ORDER BY n DESC\n",
    "  \"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce745db8-1b57-47cd-976b-4ce3ee440604",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "33oJTT7EOhHG"
   },
   "source": [
    "You can easily register dataframes as a table for Spark SQL too. So this way you can easily move between Dataframes and Spark SQL for whatever reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9007db35-22d8-4432-bcea-9bbb90017fa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gKDQHQu4OhHG",
    "outputId": "fe02617f-71c7-42ab-9773-9f075d93f37d"
   },
   "outputs": [],
   "source": [
    "# register the df we just made as a table for spark sql\n",
    "result.createOrReplaceTempView(\"result\")\n",
    "spark.sql(\"SELECT * FROM result\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24bbcd7e-55ec-4929-a898-09a27acf9269",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "58cIspoOOhHH"
   },
   "source": [
    "#### <span style=\"color:darkblue\">Question 1</span>\n",
    "\n",
    "1. Write some spark sql to get the top 'bachelor_rate' by 'education' group?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "964b6472-2d89-4158-b517-0955f1e3c587",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Pa_T7jC_OhHH",
    "outputId": "bb3324d9-1270-4f12-971c-405b3c2bf8f4"
   },
   "outputs": [],
   "source": [
    "### Question 1.1 Answer ###\n",
    "\n",
    "result = # fill in here\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82050620-e0ac-45a1-96ac-11ddce045cb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Dt6nyAlCOhHH"
   },
   "source": [
    "#### Spark DataFrames\n",
    "Below we will create our DataFrame from the SQL table and do some similar analysis as we did with Spark SQL but using the DataFrames API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbad390a-677d-4b3b-8e01-9dd7bb211ab2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "WuB1O_I0OhHH"
   },
   "outputs": [],
   "source": [
    "# register a df from the sql df\n",
    "df_adult = spark.table(\"adult\")\n",
    "cols = df_adult.columns # this will be used much later in the notebook, ignore for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da26c930-94a3-4289-ab00-61f5b283f9c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cEIO5mV4OhHH",
    "outputId": "2f3fd3c3-b86b-4790-d8a3-a037ad2a8de8"
   },
   "outputs": [],
   "source": [
    "# look at df schema\n",
    "df_adult.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0443d5cc-85c8-462c-8998-0334ac36d6c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1mJVUFnWOhHH",
    "outputId": "26204e93-bcc0-4a68-d872-da50b897a7c2"
   },
   "outputs": [],
   "source": [
    "# look at the df\n",
    "df_adult.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6f3c980-fda5-4a86-8a83-75846e041b74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "CEhyayQ2OhHH"
   },
   "source": [
    "Below we will do a similar calulation to what we did above but using the DataFrames API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "460c47b6-0405-4ce6-8015-39625fbc514a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G0pBrCOROhHH",
    "outputId": "3e49837d-e021-4a96-b497-fab3657c55ec"
   },
   "outputs": [],
   "source": [
    "# import what we will need\n",
    "from pyspark.sql.functions import when, col, mean, desc, round\n",
    "\n",
    "# wrangle the data a bit\n",
    "df_result = df_adult.select(\n",
    "  df_adult['occupation'],\n",
    "  # create a 1/0 type col on the fly\n",
    "  when( col('marital_status') == ' Divorced' , 1 ).otherwise(0).alias('is_divorced')\n",
    ")\n",
    "# do grouping (and a round)\n",
    "df_result = df_result.groupBy('occupation').agg(round(mean('is_divorced'),2).alias('divorced_rate'))\n",
    "# do ordering\n",
    "df_result = df_result.orderBy(desc('divorced_rate'))\n",
    "# show results\n",
    "df_result.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e688a7f-a03c-4c74-94ae-42d999aca3bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "O5hfkSEAOhHH"
   },
   "source": [
    "As you can see the dataframes api is a bit more verbose then just expressing what you want to do in standard SQL.<br><br>But some prefer it and might be more used to it, and there could be cases where expressing what you need to do might just be better using the DataFrame API if it is too complicated for a simple SQL expression for example of maybe involves recursion of some type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "252f073d-4272-4703-886a-4b37c9cd47f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "AOjxgxocOhHH"
   },
   "source": [
    "#### <span style=\"color:darkblue\">Question 2</span>\n",
    "1. Write some pyspark to get the top 'bachelor_rate' by 'education' group using DataFrame operations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d5c2957-a783-47ec-bcfd-a769ba0cf294",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Nh3Gr304OhHH",
    "outputId": "a19916fc-5f4d-4aed-833f-65286b15700e"
   },
   "outputs": [],
   "source": [
    "### Question 2.1 Answer ###\n",
    "\n",
    "# wrangle the data a bit\n",
    "df_result = # fill in here\n",
    "df_result.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99f244c2-4b2e-4727-8c65-66cbd75a69f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "dBW7WRRyOhHI"
   },
   "source": [
    "#### Explore & Visualize Data\n",
    "It's very easy to [collect()](https://spark.apache.org/docs/latest/rdd-programming-guide.html#printing-elements-of-an-rdd) your Spark DataFrame data into a Pandas df and then continue to analyse or plot as you might normally.\n",
    "<br><br>\n",
    "Obviously if you try to collect() a huge DataFrame then you will run into issues, so usually you would only collect aggregated or sampled data into a Pandas df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7640afab-8c43-46e2-91c6-43b5834e1a23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dfnVFmd4OhHI",
    "outputId": "282c4b57-44eb-4bb7-bad3-0741a11154bc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# do some analysis\n",
    "result = spark.sql(\n",
    "  \"\"\"\n",
    "  SELECT\n",
    "    occupation,\n",
    "    AVG(IF(income = ' >50K',1,0)) as plus_50k\n",
    "  FROM\n",
    "    adult\n",
    "  GROUP BY 1\n",
    "  ORDER BY 2 DESC\n",
    "  \"\"\")\n",
    "\n",
    "# collect results into a pandas df\n",
    "df_pandas = pd.DataFrame(\n",
    "  result.collect(),\n",
    "  columns=result.schema.names\n",
    ")\n",
    "\n",
    "# look at df\n",
    "print(df_pandas.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e7e5b12-632a-4dd9-b66b-6fb03ea902d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2QlyeoWhOhHI",
    "outputId": "3fe3c2b5-7d81-434a-aa8d-11e916b13842"
   },
   "outputs": [],
   "source": [
    "print(df_pandas.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a71f0594-35e9-480b-b3c9-7793765e3af2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1_pfoePNOhHI",
    "outputId": "8c3f9467-6fbb-4811-de82-d162af9c52b0"
   },
   "outputs": [],
   "source": [
    "print(df_pandas.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1090b4c-4789-4b83-84c7-6150b8158026",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "1utTELSLOhHI"
   },
   "source": [
    "Here we will just do some very basic plotting to show how you might collect what you are interested in into a Pandas DF and then just plot any way you normally would.\n",
    "\n",
    "For simplicity we are going to use the plotting functionality built into pandas (you could make this a pretty as you want)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "946501d8-b915-4fbd-b9cb-a0a31a802078",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "c8Ft7-iZOhHI",
    "outputId": "f520f8cb-b443-4d8f-e084-704ce0f89ee8"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# i like ggplot style\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# get simple plot on the pandas data\n",
    "myplot = df_pandas.plot(kind='barh', x='occupation', y='plus_50k')\n",
    "\n",
    "# display the plot (note - display() is a databricks function -\n",
    "# more info on plotting in Databricks is here: https://docs.databricks.com/user-guide/visualizations/matplotlib-and-ggplot.html)\n",
    "display(myplot.figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c9aafee-4f84-4f93-96ac-70a73bc2a588",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "EO3s8YwFOhHI"
   },
   "source": [
    "You can also easily get summary stats on a Spark DataFrame like below. [Here](https://databricks.com/blog/2015/06/02/statistical-and-mathematical-functions-with-dataframes-in-spark.html) is a nice blog post that has more examples.<br><br>So this is an example of why you might want to move from Spark SQL into DataFrames API as being able to just call describe() on the Spark DF is easier then trying to do the equivilant in Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2fa9c46-ff91-4424-ad28-9dd4f74f078e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tBAcsZhXOhHI",
    "outputId": "27971887-ee15-46cf-dc37-4c22f5958280"
   },
   "outputs": [],
   "source": [
    "# describe df\n",
    "df_adult.select(df_adult['age'],df_adult['education_num']).describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e7bfbde-68e3-47a3-9f3e-278bce83808b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "OmRwfTLLOhHI"
   },
   "source": [
    "### ML Pipeline - Logistic Regression vs Random Forest\n",
    "\n",
    "Below we will create two [Spark ML Pipelines](https://spark.apache.org/docs/latest/ml-pipeline.html) - one that fits a logistic regression and one that fits a random forest. We will then compare the performance of each.\n",
    "\n",
    "**Note**: A lot of the code below is adapted from [this example](https://docs.databricks.com/spark/latest/mllib/binary-classification-mllib-pipelines.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73a32ba3-ff2c-4ca0-91ca-8e6187f24363",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "rHRFY8hEOhHI"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "categoricalColumns = [\"workclass\", \"education\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native_country\"]\n",
    "stages = [] # stages in our Pipeline\n",
    "\n",
    "for categoricalCol in categoricalColumns:\n",
    "    # Category Indexing with StringIndexer\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n",
    "    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n",
    "    # encoder = OneHotEncoderEstimator(inputCol=categoricalCol + \"Index\", outputCol=categoricalCol + \"classVec\")\n",
    "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    # Add stages.  These are not run here, but will run all at once later on.\n",
    "    stages += [stringIndexer, encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2657356-694c-4a30-80bd-02ede201a55c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "p7LvBtvjOhHJ"
   },
   "outputs": [],
   "source": [
    "# Convert label into label indices using the StringIndexer\n",
    "label_stringIdx = StringIndexer(inputCol=\"income\", outputCol=\"label\")\n",
    "stages += [label_stringIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a2fa539-5109-4b69-bc89-e7a7439a1469",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "wGHHjqYcOhHJ"
   },
   "outputs": [],
   "source": [
    "# Transform all features into a vector using VectorAssembler\n",
    "numericCols = [\"age\", \"fnlwgt\", \"education_num\", \"capital_gain\", \"capital_loss\", \"hours_per_week\"]\n",
    "assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c96a56cc-57f4-4c4b-856e-f2e5701e95fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rhnO6VC-OhHJ",
    "outputId": "96d6ce1e-f829-4031-ff50-9907b4547e77"
   },
   "outputs": [],
   "source": [
    "# Create a Pipeline.\n",
    "pipeline = Pipeline(stages=stages)\n",
    "# Run the feature transformations.\n",
    "#  - fit() computes feature statistics as needed.\n",
    "#  - transform() actually transforms the features.\n",
    "pipelineModel = pipeline.fit(df_adult)\n",
    "dataset = pipelineModel.transform(df_adult)\n",
    "# Keep relevant columns\n",
    "selectedcols = [\"label\", \"features\"] + cols\n",
    "dataset = dataset.select(selectedcols)\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a5e74a5-dff1-45a3-9155-b9944d95e9aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dzh0mEF9OhHJ",
    "outputId": "3cc46046-cab8-4269-99dc-ef77fcedc788"
   },
   "outputs": [],
   "source": [
    "### Randomly split data into training and test sets. set seed for reproducibility\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed=100)\n",
    "print(trainingData.count())\n",
    "print(testData.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8de9b08-6ff6-4b6a-b67f-a78f2db52ccf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KkZAZVTgOhHJ",
    "outputId": "87cc9b28-2426-42a1-e28f-aed63c1ecad0"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# get the rate of the positive outcome from the training data to use as a threshold in the model\n",
    "training_data_positive_rate = trainingData.select(avg(trainingData['label'])).collect()[0][0]\n",
    "\n",
    "print(\"Positive rate in the training data is {}\".format(training_data_positive_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fb85c94-76bc-42aa-82b4-d00e6369e6bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "kqGhhK0dOhHJ"
   },
   "source": [
    "#### Logistic Regression - Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06e5691c-1522-4b7e-a914-57b1bfa7a3a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Su79jw6OhHJ",
    "outputId": "6757ed92-0ee5-4563-fa54-853ab8db5671"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create initial LogisticRegression model\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
    "\n",
    "# set threshold for the probability above which to predict a 1\n",
    "lr.setThreshold(training_data_positive_rate)\n",
    "# lr.setThreshold(0.5) # could use this if knew you had balanced data\n",
    "\n",
    "# Train model with Training Data\n",
    "lrModel = lr.fit(trainingData)\n",
    "\n",
    "# get training summary used for eval metrics and other params\n",
    "lrTrainingSummary = lrModel.summary\n",
    "\n",
    "# Find the best model threshold if you would like to use that instead of the empirical positve rate\n",
    "fMeasure = lrTrainingSummary.fMeasureByThreshold\n",
    "maxFMeasure = fMeasure.groupBy().max('F-Measure').select('max(F-Measure)').head()\n",
    "lrBestThreshold = fMeasure.where(fMeasure['F-Measure'] == maxFMeasure['max(F-Measure)']) \\\n",
    "    .select('threshold').head()['threshold']\n",
    "\n",
    "print(\"Best threshold based on model performance on training data is {}\".format(lrBestThreshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50f512e9-7d60-427d-8f17-530e08639605",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "-OEYLmVwOhHU"
   },
   "source": [
    "#### GBM - Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4b1128b-d223-4d3d-b202-5ad887e9cda3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "T6KqvhJVOhHU"
   },
   "source": [
    "#### <span style=\"color:darkblue\">Question 3</span>\n",
    "1. Train a GBTClassifier on the training data, call the trained model 'gbModel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "998d8fa4-1162-4f46-bd6a-40d05beadf31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "1nUt92fYOhHU",
    "outputId": "fd87012a-a121-4d6f-f75d-80096e6be49c"
   },
   "outputs": [],
   "source": [
    "### Question 3.1 Answer ###\n",
    "\n",
    "# Create initial LogisticRegression model\n",
    "gb = # fill in here\n",
    "\n",
    "# Train model with Training Data\n",
    "gbModel = # fill in here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f52bee49-0815-4252-a35f-d6a9b8b52be2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "D9RVhJFxOhHV"
   },
   "source": [
    "#### Logistic Regression - Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cb54547-1466-46ac-81bc-a9ae5d32dc5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j2Fx9TDqOhHV",
    "outputId": "c87009c4-061c-464e-b17e-bfbd4f67e60c"
   },
   "outputs": [],
   "source": [
    "# make predictions on test data\n",
    "lrPredictions = lrModel.transform(testData)\n",
    "\n",
    "# display predictions\n",
    "lrPredictions.select(\"label\", \"prediction\", \"probability\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2b53bb9-d298-4af9-bdc8-fd94a5f0b388",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "9zATqRO5OhHV"
   },
   "source": [
    "### GBM - Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f2f8c8c-070a-466c-a315-d26562880c66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "dGhbgep-OhHV"
   },
   "source": [
    "#### <span style=\"color:darkblue\">Question 4</span>\n",
    "1. Get predictions on the test data for your GBTClassifier. Call the predictions df 'gbPredictions'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88af5d4c-6bc4-45c0-a4d5-39656e5aea3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "ajQhqjFjOhHV",
    "outputId": "41e6a2b5-1d6c-4c2a-d190-90e6a3cf4ec6"
   },
   "outputs": [],
   "source": [
    "### Question 4.1 Answer ###\n",
    "\n",
    "# make predictions on test data\n",
    "gbPredictions = # fill in here\n",
    "\n",
    "gbPredictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16eabc78-ad9f-4a58-a66d-b8bdc6669417",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "qolSJv3dOhHV"
   },
   "source": [
    "#### Logistic Regression - Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3194131-7989-4854-b39a-622c6ef6f2cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "4EtPeEG3OhHV"
   },
   "source": [
    "#### <span style=\"color:darkblue\">Question 5</span>\n",
    "1. Complete the print_performance_metrics() function below to also include measures of F1, Precision, Recall, False Positive Rate and True Positive Rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e20e5588-1c65-428e-b125-9f2cb6ed8a0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "_GAQ40vSOhHV",
    "outputId": "c516f9a1-039d-47ab-ba6d-06a1eacb770d"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics, MulticlassMetrics\n",
    "\n",
    "def print_performance_metrics(predictions):\n",
    "  # Evaluate model\n",
    "  evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "  auc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
    "  aupr = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderPR\"})\n",
    "  print(\"auc = {}\".format(auc))\n",
    "  print(\"aupr = {}\".format(aupr))\n",
    "\n",
    "  # get rdd of predictions and labels for mllib eval metrics\n",
    "  predictionAndLabels = predictions.select(\"prediction\",\"label\").rdd\n",
    "\n",
    "  # Instantiate metrics objects\n",
    "  binary_metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
    "  multi_metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "  # Area under precision-recall curve\n",
    "  print(\"Area under PR = {}\".format(binary_metrics.areaUnderPR))\n",
    "  # Area under ROC curve\n",
    "  print(\"Area under ROC = {}\".format(binary_metrics.areaUnderROC))\n",
    "  # Accuracy\n",
    "  print(\"Accuracy = {}\".format(multi_metrics.accuracy))\n",
    "  # Confusion Matrix\n",
    "  print(multi_metrics.confusionMatrix())\n",
    "\n",
    "  ### Question 5.1 Answer ###\n",
    "\n",
    "  # F1\n",
    "  print(\"F1 = {}\".format(# fill in here))\n",
    "  # Precision\n",
    "  print(\"Precision = {}\".format(# fill in here))\n",
    "  # Recall\n",
    "  print(\"Recall = {}\".format(# fill in here))\n",
    "  # FPR\n",
    "  print(\"FPR = {}\".format(# fill in here))\n",
    "  # TPR\n",
    "  print(\"TPR = {}\".format(# fill in here))\n",
    "\n",
    "\n",
    "print_performance_metrics(lrPredictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21aa0bfe-9510-470d-9692-7c252c9bc58f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "kaDztHZXOhHV"
   },
   "source": [
    "#### GBM - Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "111e82b8-8470-42d0-9485-5fdb59912e56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "wwegtB7aOhHV",
    "outputId": "34238670-53a5-4479-8db1-bbffbd34ad2c"
   },
   "outputs": [],
   "source": [
    "print_performance_metrics(gbPredictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9d7ac8b-69d9-47d4-9412-9342faad7c05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "qU2qwawbOhHV"
   },
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "766d216e-da4c-4d99-8693-44c74ef75370",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "XK-l_CWYOhHV"
   },
   "source": [
    "For each model you can run the below comand to see its params and a brief explanation of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcfef6fc-f469-4556-bebc-9ed17b605b8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H0KoAfE-OhHV",
    "outputId": "90e2e5dc-d4b2-4e28-e9fb-225ee1c27bc9"
   },
   "outputs": [],
   "source": [
    "print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "077280d3-39ad-4b29-b97e-ca45993c7527",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "aB58DaNOOhHV",
    "outputId": "6c3c9549-ffd2-4d66-cf21-374927b92ae8"
   },
   "outputs": [],
   "source": [
    "print(gb.explainParams())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5daa3f89-6335-47e0-96e6-b1e0c43a3aea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "hyHJ08QJOhHV"
   },
   "source": [
    "#### Logisitic Regression - Param Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d0ad441-c23a-484b-81e4-14b7fbf0b27e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "22JrgPAbOhHW"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "lrParamGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "             .addGrid(lr.maxIter, [2, 5])\n",
    "             .build())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b72cc844-4efe-498a-aff0-18403c629de6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "meovwY1kOhHW"
   },
   "source": [
    "#### GBM - Param Grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47696aef-d15e-4885-94aa-e263c7a56121",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "p6JR-IgtOhHW"
   },
   "source": [
    "#### <span style=\"color:darkblue\">Question 6</span>\n",
    "1. Build out a param grid for the gb model, call it 'gbParamGrid'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34b95f49-81f1-4edc-949d-e453c6214d22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "aVObDX9_OhHW",
    "outputId": "333aa0cd-bcbf-44e0-e07c-8328e9bd79fe"
   },
   "outputs": [],
   "source": [
    "### Question 6.1 Answer ###\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "gbParamGrid = # fill in here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb7eecc2-5510-4563-8fc1-ec0f8efa7ecd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "lmCsUUr_OhHW"
   },
   "source": [
    "#### Logistic Regression - Perform Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d2856c2-cec9-43e5-8b42-61f93429b97f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "9uoCe7qqOhHW"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics, MulticlassMetrics\n",
    "\n",
    "# set up an evaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "\n",
    "# Create CrossValidator\n",
    "lrCv = CrossValidator(estimator=lr, estimatorParamMaps=lrParamGrid, evaluator=evaluator, numFolds=2)\n",
    "\n",
    "# Run cross validations\n",
    "lrCvModel = lrCv.fit(trainingData)\n",
    "# this will likely take a fair amount of time because of the amount of models that we're creating and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "919f51ca-6b9d-43ac-a32b-16bf811f3a0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tyrY3FBdOhHW",
    "outputId": "c60b7e47-9ffa-4d1b-811a-8aebefc759bc"
   },
   "outputs": [],
   "source": [
    "# below approach to getting at the best params from the best cv model taken from:\n",
    "# https://stackoverflow.com/a/46353730/1919374\n",
    "\n",
    "# look at best params from the CV\n",
    "print(lrCvModel.bestModel._java_obj.getRegParam())\n",
    "print(lrCvModel.bestModel._java_obj.getElasticNetParam())\n",
    "print(lrCvModel.bestModel._java_obj.getMaxIter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea6bd5d5-df8c-4eaa-aa24-7dcc2e7627cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "wkRvnEZAOhHW"
   },
   "source": [
    "#### GBM - Perform Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb2030b9-f96b-4402-b6cf-ab05effc3944",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "HRIbFcdEOhHW"
   },
   "source": [
    "#### <span style=\"color:darkblue\">Question 7</span>\n",
    "1. Perform cross validation of params on your 'gb' model.\n",
    "1. Print out the best params you found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98a9e5e6-449d-4f6e-8fee-3fc3e1e56c2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "6DNPyUl8OhHW",
    "outputId": "7c4872d3-b0d9-4c51-be24-f4f38c19a4f1"
   },
   "outputs": [],
   "source": [
    "### Question 7.1 Answer ###\n",
    "\n",
    "# Create CrossValidator\n",
    "gbCv = # fill in here\n",
    "\n",
    "# Run cross validations\n",
    "gbCvModel = # fill in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60e1eff8-e84e-4a8b-bab1-3b5ce2e5e9f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "DrUyhU5LOhHW",
    "outputId": "94133809-3309-44c7-85c4-75ca1f1dcead"
   },
   "outputs": [],
   "source": [
    "### Question 7.2 Answer ###\n",
    "\n",
    "# look at best params from the CV\n",
    "print(# fill in here)\n",
    "print(# fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efd7e902-4c5c-48ca-8dfc-93a8ca1192e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "KMuEADRmOhHW"
   },
   "source": [
    "#### Logistic Regression - CV Model Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "402209de-a5da-4195-ac8a-fe20fb1c9b87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ZxgV4XCOhHW",
    "outputId": "ffc03e7d-328e-4987-bacb-598bfce70fd5"
   },
   "outputs": [],
   "source": [
    "# Use test set to measure the accuracy of our model on new data\n",
    "lrCvPredictions = lrCvModel.transform(testData)\n",
    "\n",
    "lrCvPredictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cff1a654-95f5-4e08-9705-76e133141de7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "URk4kbLqOhHW"
   },
   "source": [
    "#### GBM - CV Model Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7e0cddb-99ed-4fe9-9c46-ed9dc12f5044",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "oUZ3wcakOhHW",
    "outputId": "2b1301f6-55e7-4acf-82b8-0ada23dd6bfa"
   },
   "outputs": [],
   "source": [
    "gbCvPredictions = gbCvModel.transform(testData)\n",
    "\n",
    "gbCvPredictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9db16de-2dcd-45ee-9e00-50009530a361",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "q6DUHdzsOhHW"
   },
   "source": [
    "#### Logistic Regression - CV Model Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30e58e24-4d40-4c50-bbb0-73e059994495",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "vu7mBs2cOhHW",
    "outputId": "f5d052a4-cb6d-4fc4-a308-40648c2e7516"
   },
   "outputs": [],
   "source": [
    "print_performance_metrics(lrCvPredictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f959c275-36a7-42a3-b1b7-551e28e651c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "FqSJkAoPOhHX"
   },
   "source": [
    "#### GBM - CV Model Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62cfc105-53b6-4690-b732-e6446b965087",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "OGQqyXKZOhHX",
    "outputId": "c6c194d5-09fb-4057-cf80-c7772dee4ed0"
   },
   "outputs": [],
   "source": [
    "print_performance_metrics(gbCvPredictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39417456-a688-48c7-9aa4-58801fbf147e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "2NPLD6wNOhHX"
   },
   "source": [
    "#### Logistic Regression - Model Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f567014e-5897-4b4f-b8a5-0aa1b536fe9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tS28r3jOOhHX",
    "outputId": "565c2a95-0275-4db7-bc03-7f8002eb853f"
   },
   "outputs": [],
   "source": [
    "print('Model Intercept: ', lrCvModel.bestModel.intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5f8f9be-7672-4cee-83e1-3f72e632a07f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xZr143XJOhHX",
    "outputId": "e5675520-2c86-4a29-d5a4-d668591564a3"
   },
   "outputs": [],
   "source": [
    "lrWeights = lrCvModel.bestModel.coefficients\n",
    "lrWeights = [(float(w),) for w in lrWeights]  # convert numpy type to float, and to tuple\n",
    "lrWeightsDF = spark.createDataFrame(lrWeights, [\"Feature Weight\"])\n",
    "lrWeightsDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "857e8f3b-45fb-4af6-b8ce-dd30d3190095",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Hz7nxFIAOhHX"
   },
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03af6259-aa64-4309-a2a8-6207c8675e26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "USPRL7NTOhHX"
   },
   "source": [
    "#### <span style=\"color:darkblue\">Question 8</span>\n",
    "1. Print out a table of feature_name and feature_coefficient from the Logistic Regression model.\n",
    "<br><br>\n",
    "Hint: Adapt the code from here: https://stackoverflow.com/questions/42935914/how-to-map-features-from-the-output-of-a-vectorassembler-back-to-the-column-name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb525282-a308-446e-a14a-37fae0a9019d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "KXaTApT-OhHX",
    "outputId": "2c0c84f2-f7ec-459e-9901-afd52176b656"
   },
   "outputs": [],
   "source": [
    "### Question 8.1 Answer ###\n",
    "\n",
    "# from: https://stackoverflow.com/questions/42935914/how-to-map-features-from-the-output-of-a-vectorassembler-back-to-the-column-name\n",
    "\n",
    "# fill in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "432159f9-6383-42eb-8ced-489f722d68e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Y_tZAKZmOhHX",
    "outputId": "1f67304a-dad8-4a78-9ecd-2151462b8470"
   },
   "outputs": [],
   "source": [
    "gbCvFeatureImportance = pd.DataFrame([(name, gbCvModel.bestModel.featureImportances[idx]) for idx, name in attrs],columns=['feature_name','feature_importance'])\n",
    "\n",
    "print(gbCvFeatureImportance.sort_values(by=['feature_importance'],ascending =False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd47490f-2880-4b55-9b80-e74faa0a7fef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "QQAPcm6UOhHX"
   },
   "source": [
    "#### <span style=\"color:darkblue\">Question 9</span>\n",
    "1. Build and train a RandomForestClassifier and print out a table of feature importances from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d63f4af-4566-4657-8f7e-a6b848f03356",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "mjGMNuV5OhHX",
    "outputId": "28377b3c-7770-492e-f5f6-c0be59799fa0"
   },
   "outputs": [],
   "source": [
    "### Question 9.1 Answer ###\n",
    "\n",
    "from pyspark.ml.classification import # fill in here\n",
    "\n",
    "rf = # fill in here\n",
    "\n",
    "rfModel = # fill in here\n",
    "\n",
    "rfFeatureImportance = # fill in here\n",
    "\n",
    "print(# fill in here)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Copy_of_Spark_DF_SQL_ML_Exercise",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
